{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0s7xHqya11Fr",
    "outputId": "5ea7b3a2-6b0c-4e38-d7fa-79278b9f7719"
   },
   "outputs": [],
   "source": [
    "# # Note: This section is intended for use in Google Colab only.\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define the repository URLs and set the home directories based on Colab's file structure\n",
    "# repo = {\"url\": \"https://github.com/noye09/cs612_SMU_g4.git\", \"target_dir\": \"/content/cs612_SMU_g4\"}\n",
    "\n",
    "\n",
    "# # Clone the repositories if they don't exist\n",
    "# if not os.path.isdir(repo[\"target_dir\"]):\n",
    "#     print(f\"Cloning repository from {repo['url']}...\")\n",
    "#     !git clone {repo['url']}\n",
    "\n",
    "#     # Rename folder if needed (e.g., replacing hyphens with underscores)\n",
    "#     if repo[\"url\"].endswith(\"badnets-pytorch.git\"):\n",
    "#         shutil.move(\"badnets-pytorch\", repo[\"target_dir\"])\n",
    "# else:\n",
    "#     print(f\"Repository already cloned at {repo['target_dir']}!\")\n",
    "\n",
    "# # Set the working directory to the desired location\n",
    "# HOME_DIR = \"/content/cs612_SMU_g4/backdoor_cs612/\"\n",
    "# if os.path.isdir(HOME_DIR):\n",
    "#     os.chdir(HOME_DIR)\n",
    "#     print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "# else:\n",
    "#     print(f\"Directory '{HOME_DIR}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njk_K1vw143u",
    "outputId": "27a9ac35-027d-496d-b2c1-75c1fa91cc31"
   },
   "outputs": [],
   "source": [
    "# TriggerOptimizer\n",
    "# 'initialize_model_and_data' initializes and loads the backdoor model\n",
    "# 'detect_backdoor' runs the backdoor detection process\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#import matplotlib.pyplot as plt  # For debugging and visualization\n",
    "\n",
    "from model_class.model_mnist import MNISTNet  # Import MNIST model class from model_mnist.py\n",
    "from model_class.model_cifar10 import CIFAR10Net  # Import CIFAR-10 model class from model_cifar10.py\n",
    "from model_class.badnet import BadNet\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "class TriggerOptimizer:\n",
    "    def __init__(self, model, target_class, dataset, input_shape, device='cpu', lr=0.0005):\n",
    "        # Initialize model, parameters, and optimizer settings\n",
    "        self.model = model\n",
    "        self.target_class = target_class\n",
    "        self.dataset = dataset\n",
    "        self.input_shape = input_shape\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "\n",
    "        # Initialize mask and pattern\n",
    "        self.mask = torch.rand(input_shape, requires_grad=True, device=device)\n",
    "        self.pattern = torch.rand(input_shape, requires_grad=True, device=device)\n",
    "        self.optimizer = optim.Adam([self.mask, self.pattern], lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=1500, gamma=0.5)  # Adjusted schedule\n",
    "\n",
    "    def optimize(self, max_iters=5000, patience=300, batch_size=10):\n",
    "        \"\"\"\n",
    "        Generate a minimal trigger mask and pattern for the specified target class.\n",
    "        \"\"\"\n",
    "        # Shuffle dataset to ensure diversity\n",
    "        dataset_indices = torch.randperm(len(self.dataset))\n",
    "\n",
    "        # Find multiple images belonging to the target class\n",
    "        samples = []\n",
    "        for idx in dataset_indices:\n",
    "            img, lbl = self.dataset[idx]\n",
    "            if lbl == self.target_class:\n",
    "                samples.append(img)\n",
    "            if len(samples) >= batch_size:\n",
    "                break\n",
    "\n",
    "        if len(samples) == 0:\n",
    "            raise ValueError(f\"No samples found for target class {self.target_class}\")\n",
    "\n",
    "        sample_inputs = torch.stack(samples).to(self.device)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        counter = 0\n",
    "\n",
    "        for i in range(max_iters):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Apply mask and pattern on each sample input in the batch\n",
    "            masked_inputs = self.pattern * self.mask + sample_inputs * (1 - self.mask)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(masked_inputs)  # Get predictions for all samples\n",
    "\n",
    "            # Calculate the loss (encourage model to predict the target class)\n",
    "            target_labels = torch.full((batch_size,), self.target_class, dtype=torch.long, device=self.device)\n",
    "            target_loss = nn.CrossEntropyLoss()(outputs, target_labels)\n",
    "\n",
    "            # Regularization to minimize the mask area (encourage sparse mask)\n",
    "            mask_penalty = self.mask.abs().mean()\n",
    "            total_loss = target_loss + 0.001 * mask_penalty\n",
    "\n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # Clip mask to keep values between 0 and 1\n",
    "            self.mask.data = torch.clamp(self.mask.data, 0, 1)\n",
    "\n",
    "            # Early stopping condition\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at iteration {i} due to no improvement\")\n",
    "                    break\n",
    "\n",
    "            # Print progress every 500 iterations\n",
    "            if i % 500 == 0:\n",
    "                print(f\"Iteration {i}/{max_iters}, Loss: {total_loss.item()}\")\n",
    "\n",
    "        return self.mask.detach(), self.pattern.detach()\n",
    "\n",
    "\n",
    "def initialize_model_and_data(model_selection, model_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Initializes the model and loads data based on the model selection.\n",
    "    \"\"\"\n",
    "    DATASET_OPTIONS = {\n",
    "    'mnist': {\n",
    "        'model_class': MNISTNet,\n",
    "        'dataset_class': datasets.MNIST,\n",
    "        'model_kwargs': {}\n",
    "    },\n",
    "    'cifar10': {\n",
    "        'model_class': CIFAR10Net,\n",
    "        'dataset_class': datasets.CIFAR10,\n",
    "        'model_kwargs': {}\n",
    "    },\n",
    "    'badnets_pytorch_mnist': {\n",
    "        'model_class': BadNet,\n",
    "        'dataset_class': datasets.MNIST,\n",
    "        'model_kwargs': {'input_channels': 1, 'output_num': 10}\n",
    "    },\n",
    "    'badnets_pytorch_cifar10': {\n",
    "        'model_class': BadNet,\n",
    "        'dataset_class': datasets.CIFAR10,\n",
    "        'model_kwargs': {'input_channels': 3, 'output_num': 10}\n",
    "    }\n",
    "}\n",
    "\n",
    "    # Initialize the model and load data based on the model selection\n",
    "    if model_selection not in DATASET_OPTIONS:\n",
    "        raise ValueError(f\"Invalid model selection '{model_selection}'. Available options: {list(DATASET_OPTIONS.keys())}\")\n",
    "\n",
    "    model_class = DATASET_OPTIONS[model_selection]['model_class']\n",
    "    dataset_class = DATASET_OPTIONS[model_selection]['dataset_class']\n",
    "    model_kwargs = DATASET_OPTIONS[model_selection]['model_kwargs']\n",
    "\n",
    "    # Initialize model with possible arguments\n",
    "    model = model_class(**model_kwargs)\n",
    "    test_dataset = dataset_class(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    # Load the model weights\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at path: {model_path}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)  # Send model to specified device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    return model, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "def detect_backdoor(model, dataset, num_classes, device='cpu'):\n",
    "    \"\"\"\n",
    "    Detects potential backdoors in the model using Neural Cleanse.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    input_shape = dataset[0][0].shape  # Get the shape of a single input image\n",
    "\n",
    "    results = {\n",
    "        \"backdoor_detected\": False,\n",
    "        \"anomaly_scores\": {},\n",
    "        \"triggers\": {}\n",
    "    }\n",
    "\n",
    "    mask_sizes = []\n",
    "\n",
    "    for target_class in range(num_classes):\n",
    "        try:\n",
    "            # Generate a trigger for each class\n",
    "            optimizer = TriggerOptimizer(model, target_class, dataset, input_shape, device=device)\n",
    "            mask, pattern = optimizer.optimize(max_iters=3000)\n",
    "\n",
    "            # Calculate the size of the mask (used to detect anomalies)\n",
    "            mask_size = mask.abs().mean().item()\n",
    "            mask_sizes.append(mask_size)\n",
    "\n",
    "            # Store the mask and pattern for potential backdoor classes\n",
    "            results[\"triggers\"][target_class] = {\n",
    "                \"mask\": mask.cpu().numpy(),\n",
    "                \"pattern\": pattern.cpu().numpy(),\n",
    "                \"mask_size\": mask_size\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle cases where mask generation fails\n",
    "            print(f\"Error processing target class {target_class}: {e}\")\n",
    "            mask_sizes.append(np.nan)\n",
    "            results[\"triggers\"][target_class] = {\n",
    "                \"mask\": None,\n",
    "                \"pattern\": None,\n",
    "                \"mask_size\": np.nan\n",
    "            }\n",
    "\n",
    "    # Calculate anomaly scores for each class\n",
    "    valid_mask_sizes = [x for x in mask_sizes if not np.isnan(x)]\n",
    "    mean_size = np.mean(valid_mask_sizes)\n",
    "    std_size = np.std(valid_mask_sizes)\n",
    "\n",
    "    for target_class, mask_size in enumerate(mask_sizes):\n",
    "        if not np.isnan(mask_size):\n",
    "            anomaly_score = (mask_size - mean_size) / (std_size + 1e-6)  # Avoid division by zero\n",
    "            results[\"anomaly_scores\"][target_class] = anomaly_score\n",
    "\n",
    "            # If the anomaly score is below a certain threshold, flag it as a backdoor\n",
    "            if anomaly_score < -2:  # Typically, a threshold like -2 or -3 standard deviations\n",
    "                results[\"backdoor_detected\"] = True\n",
    "                results[\"target_class\"] = target_class\n",
    "        else:\n",
    "            results[\"anomaly_scores\"][target_class] = float('inf')\n",
    "    #visualize_generated_masks(results)\n",
    "    return results\n",
    "\n",
    "# def visualize_generated_masks(results):\n",
    "#     \"\"\"\n",
    "#     Visualizes the generated masks and patterns for each target class.\n",
    "#     \"\"\"\n",
    "#     for target_class, trigger_info in results[\"triggers\"].items():\n",
    "#         if trigger_info[\"mask\"] is None:\n",
    "#             continue\n",
    "\n",
    "#         mask = trigger_info[\"mask\"]\n",
    "#         pattern = trigger_info[\"pattern\"]\n",
    "\n",
    "#         # Plot the mask and pattern\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "\n",
    "#         # Plot the mask\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         if len(mask.shape) == 3:  # If the mask has 3 dimensions\n",
    "#             mask = mask[0]  # Take the first channel (assuming it's the same across channels)\n",
    "#         plt.imshow(mask, cmap='gray')\n",
    "#         plt.title(f\"Mask for Target Class {target_class}\")\n",
    "#         plt.axis('off')\n",
    "\n",
    "#         # Plot the pattern\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         if pattern.shape[0] == 3:  # RGB Image, shape is (C, H, W), need to convert to (H, W, C)\n",
    "#             pattern = np.transpose(pattern, (1, 2, 0))  # Transpose from (C, H, W) to (H, W, C)\n",
    "#             plt.imshow(pattern)\n",
    "#         else:  # Grayscale\n",
    "#             plt.imshow(pattern.squeeze(), cmap='gray')\n",
    "#         plt.title(f\"Pattern for Target Class {target_class}\")\n",
    "#         plt.axis('off')\n",
    "\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Model Selection = 'mnist', Model Path = './model7/mnist_bd_50_c5.pt'\n",
      "Active device: cuda\n",
      "Run 1/10\n",
      "Iteration 0/3000, Loss: 2.2853918075561523\n",
      "Iteration 500/3000, Loss: 0.004779326729476452\n",
      "Iteration 1000/3000, Loss: 0.00201999768614769\n",
      "Iteration 1500/3000, Loss: 0.0012823848519474268\n",
      "Iteration 2000/3000, Loss: 0.001100517576560378\n",
      "Iteration 2500/3000, Loss: 0.0009644479723647237\n",
      "Iteration 0/3000, Loss: 8.958544731140137\n",
      "Iteration 500/3000, Loss: 0.03188387677073479\n",
      "Iteration 1000/3000, Loss: 0.010586570017039776\n",
      "Iteration 1500/3000, Loss: 0.006145291030406952\n",
      "Iteration 2000/3000, Loss: 0.00504390150308609\n",
      "Iteration 2500/3000, Loss: 0.003975762985646725\n",
      "Iteration 0/3000, Loss: 2.998663902282715\n",
      "Iteration 500/3000, Loss: 0.02379404567182064\n",
      "Iteration 1000/3000, Loss: 0.006359452847391367\n",
      "Iteration 1500/3000, Loss: 0.003154417034238577\n",
      "Iteration 2000/3000, Loss: 0.002427831059321761\n",
      "Iteration 2500/3000, Loss: 0.0019084373489022255\n",
      "Iteration 0/3000, Loss: 1.4867579936981201\n",
      "Iteration 500/3000, Loss: 0.00359896756708622\n",
      "Iteration 1000/3000, Loss: 0.0019110268913209438\n",
      "Iteration 1500/3000, Loss: 0.0014666970819234848\n",
      "Iteration 2000/3000, Loss: 0.0013372718822211027\n",
      "Iteration 2500/3000, Loss: 0.0012128292582929134\n",
      "Iteration 0/3000, Loss: 13.574151992797852\n",
      "Iteration 500/3000, Loss: 0.23616015911102295\n",
      "Iteration 1000/3000, Loss: 0.07520969212055206\n",
      "Iteration 1500/3000, Loss: 0.03843243420124054\n",
      "Iteration 2000/3000, Loss: 0.028999371454119682\n",
      "Iteration 2500/3000, Loss: 0.02182679995894432\n",
      "Iteration 0/3000, Loss: 0.3534412086009979\n",
      "Iteration 500/3000, Loss: 0.001007123850286007\n",
      "Iteration 1000/3000, Loss: 0.0007092910818755627\n",
      "Iteration 1500/3000, Loss: 0.0006256392225623131\n",
      "Iteration 2000/3000, Loss: 0.0006015390390530229\n",
      "Iteration 2500/3000, Loss: 0.0005819491925649345\n",
      "Iteration 0/3000, Loss: 4.423340797424316\n",
      "Iteration 500/3000, Loss: 0.011078044772148132\n",
      "Iteration 1000/3000, Loss: 0.0042969114147126675\n",
      "Iteration 1500/3000, Loss: 0.0027283616364002228\n",
      "Iteration 2000/3000, Loss: 0.0022841673344373703\n",
      "Iteration 2500/3000, Loss: 0.0019190593156963587\n",
      "Iteration 0/3000, Loss: 8.27359676361084\n",
      "Iteration 500/3000, Loss: 0.4162815511226654\n",
      "Iteration 1000/3000, Loss: 0.21308644115924835\n",
      "Iteration 1500/3000, Loss: 0.12398982793092728\n",
      "Iteration 2000/3000, Loss: 0.09592564404010773\n",
      "Iteration 2500/3000, Loss: 0.07326871901750565\n",
      "Iteration 0/3000, Loss: 4.831841945648193\n",
      "Iteration 500/3000, Loss: 0.006546442396938801\n",
      "Iteration 1000/3000, Loss: 0.002655371092259884\n",
      "Iteration 1500/3000, Loss: 0.0016828898806124926\n",
      "Iteration 2000/3000, Loss: 0.0014166560722514987\n",
      "Iteration 2500/3000, Loss: 0.00122160988394171\n",
      "Iteration 0/3000, Loss: 8.811041831970215\n",
      "Iteration 500/3000, Loss: 0.2653331756591797\n",
      "Iteration 1000/3000, Loss: 0.11106586456298828\n",
      "Iteration 1500/3000, Loss: 0.05450912192463875\n",
      "Iteration 2000/3000, Loss: 0.03915560245513916\n",
      "Iteration 2500/3000, Loss: 0.02655154839158058\n",
      "Run 2/10\n",
      "Iteration 0/3000, Loss: 0.08878552168607712\n",
      "Iteration 500/3000, Loss: 0.0009268841240555048\n",
      "Iteration 1000/3000, Loss: 0.0007302120793610811\n",
      "Iteration 1500/3000, Loss: 0.0006459197029471397\n",
      "Iteration 2000/3000, Loss: 0.0006175006274133921\n",
      "Iteration 2500/3000, Loss: 0.0005924743018113077\n",
      "Iteration 0/3000, Loss: 10.960434913635254\n",
      "Iteration 500/3000, Loss: 0.06973487883806229\n",
      "Iteration 1000/3000, Loss: 0.015572287142276764\n",
      "Iteration 1500/3000, Loss: 0.0075082965195178986\n",
      "Iteration 2000/3000, Loss: 0.0057007987052202225\n",
      "Iteration 2500/3000, Loss: 0.0043230606243014336\n",
      "Iteration 0/3000, Loss: 5.666709899902344\n",
      "Iteration 500/3000, Loss: 0.031135477125644684\n",
      "Iteration 1000/3000, Loss: 0.015223315916955471\n",
      "Iteration 1500/3000, Loss: 0.008773052133619785\n",
      "Iteration 2000/3000, Loss: 0.005872658453881741\n",
      "Iteration 2500/3000, Loss: 0.003925366792827845\n",
      "Iteration 0/3000, Loss: 1.8561559915542603\n",
      "Iteration 500/3000, Loss: 0.004776684567332268\n",
      "Iteration 1000/3000, Loss: 0.0021428747568279505\n",
      "Iteration 1500/3000, Loss: 0.0013832257827743888\n",
      "Iteration 2000/3000, Loss: 0.0011746140662580729\n",
      "Iteration 2500/3000, Loss: 0.0010140968952327967\n",
      "Iteration 0/3000, Loss: 8.997566223144531\n",
      "Iteration 500/3000, Loss: 0.0241035558283329\n",
      "Iteration 1000/3000, Loss: 0.008259199559688568\n",
      "Iteration 1500/3000, Loss: 0.0047835479490458965\n",
      "Iteration 2000/3000, Loss: 0.0038746295031160116\n",
      "Iteration 2500/3000, Loss: 0.003145279362797737\n",
      "Iteration 0/3000, Loss: 0.1979341059923172\n",
      "Iteration 500/3000, Loss: 0.0009361838456243277\n",
      "Iteration 1000/3000, Loss: 0.000650156638585031\n",
      "Iteration 1500/3000, Loss: 0.000576544611249119\n",
      "Iteration 2000/3000, Loss: 0.0005553343798965216\n",
      "Iteration 2500/3000, Loss: 0.0005382868694141507\n",
      "Iteration 0/3000, Loss: 7.834455966949463\n",
      "Iteration 500/3000, Loss: 0.011812293902039528\n",
      "Iteration 1000/3000, Loss: 0.004204919561743736\n",
      "Iteration 1500/3000, Loss: 0.002485426841303706\n",
      "Iteration 2000/3000, Loss: 0.0020404222887009382\n",
      "Iteration 2500/3000, Loss: 0.0017024088883772492\n",
      "Iteration 0/3000, Loss: 7.982412815093994\n",
      "Iteration 500/3000, Loss: 0.019980739802122116\n",
      "Iteration 1000/3000, Loss: 0.007728138938546181\n",
      "Iteration 1500/3000, Loss: 0.004392352886497974\n",
      "Iteration 2000/3000, Loss: 0.0034622568637132645\n",
      "Iteration 2500/3000, Loss: 0.002745778765529394\n",
      "Iteration 0/3000, Loss: 0.739490270614624\n",
      "Iteration 500/3000, Loss: 0.00200393283739686\n",
      "Iteration 1000/3000, Loss: 0.0011595010291785002\n",
      "Iteration 1500/3000, Loss: 0.0008879255037754774\n",
      "Iteration 2000/3000, Loss: 0.0008046942530199885\n",
      "Iteration 2500/3000, Loss: 0.0007376943249255419\n",
      "Iteration 0/3000, Loss: 6.965510368347168\n",
      "Iteration 500/3000, Loss: 0.30507805943489075\n",
      "Iteration 1000/3000, Loss: 0.1172865703701973\n",
      "Iteration 1500/3000, Loss: 0.04463224112987518\n",
      "Iteration 2000/3000, Loss: 0.029779477044939995\n",
      "Iteration 2500/3000, Loss: 0.02065950632095337\n",
      "Run 3/10\n",
      "Iteration 0/3000, Loss: 0.18246448040008545\n",
      "Iteration 500/3000, Loss: 0.0013122010277584195\n",
      "Iteration 1000/3000, Loss: 0.0008977821562439203\n",
      "Iteration 1500/3000, Loss: 0.0007348667713813484\n",
      "Iteration 2000/3000, Loss: 0.0006851066136732697\n",
      "Iteration 2500/3000, Loss: 0.0006426980253309011\n",
      "Iteration 0/3000, Loss: 7.2640838623046875\n",
      "Iteration 500/3000, Loss: 0.055629950016736984\n",
      "Iteration 1000/3000, Loss: 0.01204526238143444\n",
      "Iteration 1500/3000, Loss: 0.005810078699141741\n",
      "Iteration 2000/3000, Loss: 0.00451813917607069\n",
      "Iteration 2500/3000, Loss: 0.003659492125734687\n",
      "Iteration 0/3000, Loss: 6.246743202209473\n",
      "Iteration 500/3000, Loss: 0.03717392310500145\n",
      "Iteration 1000/3000, Loss: 0.015262437053024769\n",
      "Iteration 1500/3000, Loss: 0.008240093477070332\n",
      "Iteration 2000/3000, Loss: 0.0054734498262405396\n",
      "Iteration 2500/3000, Loss: 0.003764985827729106\n",
      "Iteration 0/3000, Loss: 3.9726383686065674\n",
      "Iteration 500/3000, Loss: 0.009612539783120155\n",
      "Iteration 1000/3000, Loss: 0.0035357384476810694\n",
      "Iteration 1500/3000, Loss: 0.002222698414698243\n",
      "Iteration 2000/3000, Loss: 0.0018575070425868034\n",
      "Iteration 2500/3000, Loss: 0.0015836723614484072\n",
      "Iteration 0/3000, Loss: 12.525336265563965\n",
      "Iteration 500/3000, Loss: 0.029873207211494446\n",
      "Iteration 1000/3000, Loss: 0.00894314143806696\n",
      "Iteration 1500/3000, Loss: 0.0047782305628061295\n",
      "Iteration 2000/3000, Loss: 0.0037359967827796936\n",
      "Iteration 2500/3000, Loss: 0.002969857770949602\n",
      "Iteration 0/3000, Loss: 0.0387725830078125\n",
      "Iteration 500/3000, Loss: 0.000567691691685468\n",
      "Iteration 1000/3000, Loss: 0.0005196967395022511\n",
      "Iteration 1500/3000, Loss: 0.000500038149766624\n",
      "Iteration 2000/3000, Loss: 0.0004914869205094874\n",
      "Iteration 2500/3000, Loss: 0.0004823050694540143\n",
      "Iteration 0/3000, Loss: 4.429755210876465\n",
      "Iteration 500/3000, Loss: 0.10271688550710678\n",
      "Iteration 1000/3000, Loss: 0.026804322376847267\n",
      "Iteration 1500/3000, Loss: 0.014104236848652363\n",
      "Iteration 2000/3000, Loss: 0.011105090379714966\n",
      "Iteration 2500/3000, Loss: 0.008744030259549618\n",
      "Iteration 0/3000, Loss: 8.423722267150879\n",
      "Iteration 500/3000, Loss: 0.022339366376399994\n",
      "Iteration 1000/3000, Loss: 0.011179884895682335\n",
      "Iteration 1500/3000, Loss: 0.006891638971865177\n",
      "Iteration 2000/3000, Loss: 0.005641028750687838\n",
      "Iteration 2500/3000, Loss: 0.004702823702245951\n",
      "Iteration 0/3000, Loss: 0.9386721253395081\n",
      "Iteration 500/3000, Loss: 0.0029587417375296354\n",
      "Iteration 1000/3000, Loss: 0.0014421893283724785\n",
      "Iteration 1500/3000, Loss: 0.0010365140624344349\n",
      "Iteration 2000/3000, Loss: 0.0009179039625450969\n",
      "Iteration 2500/3000, Loss: 0.0008245025528594851\n",
      "Iteration 0/3000, Loss: 8.34444808959961\n",
      "Iteration 500/3000, Loss: 0.6174659729003906\n",
      "Iteration 1000/3000, Loss: 0.11331599205732346\n",
      "Iteration 1500/3000, Loss: 0.04889789968729019\n",
      "Iteration 2000/3000, Loss: 0.034907545894384384\n",
      "Iteration 2500/3000, Loss: 0.02513188309967518\n",
      "Run 4/10\n",
      "Iteration 0/3000, Loss: 0.3758116662502289\n",
      "Iteration 500/3000, Loss: 0.001566205290146172\n",
      "Iteration 1000/3000, Loss: 0.0009872704977169633\n",
      "Iteration 1500/3000, Loss: 0.0008092725183814764\n",
      "Iteration 2000/3000, Loss: 0.0007514063618145883\n",
      "Iteration 2500/3000, Loss: 0.0007032113499008119\n",
      "Iteration 0/3000, Loss: 7.1506757736206055\n",
      "Iteration 500/3000, Loss: 0.028648538514971733\n",
      "Iteration 1000/3000, Loss: 0.00904469657689333\n",
      "Iteration 1500/3000, Loss: 0.005505955778062344\n",
      "Iteration 2000/3000, Loss: 0.004537296947091818\n",
      "Iteration 2500/3000, Loss: 0.0037984359078109264\n",
      "Iteration 0/3000, Loss: 4.649072170257568\n",
      "Iteration 500/3000, Loss: 0.04224018007516861\n",
      "Iteration 1000/3000, Loss: 0.021410031244158745\n",
      "Iteration 1500/3000, Loss: 0.005707406904548407\n",
      "Iteration 2000/3000, Loss: 0.003933801781386137\n",
      "Iteration 2500/3000, Loss: 0.002807456534355879\n",
      "Iteration 0/3000, Loss: 6.538052558898926\n",
      "Iteration 500/3000, Loss: 0.010080600157380104\n",
      "Iteration 1000/3000, Loss: 0.003459461499005556\n",
      "Iteration 1500/3000, Loss: 0.0020385938696563244\n",
      "Iteration 2000/3000, Loss: 0.0017282472690567374\n",
      "Iteration 2500/3000, Loss: 0.0014881971292197704\n",
      "Iteration 0/3000, Loss: 11.547154426574707\n",
      "Iteration 500/3000, Loss: 0.030115701258182526\n",
      "Iteration 1000/3000, Loss: 0.009424908086657524\n",
      "Iteration 1500/3000, Loss: 0.0050135403871536255\n",
      "Iteration 2000/3000, Loss: 0.003928213380277157\n",
      "Iteration 2500/3000, Loss: 0.003126914845779538\n",
      "Iteration 0/3000, Loss: 0.05591069906949997\n",
      "Iteration 500/3000, Loss: 0.0011014307383447886\n",
      "Iteration 1000/3000, Loss: 0.0006407468463294208\n",
      "Iteration 1500/3000, Loss: 0.0005606284248642623\n",
      "Iteration 2000/3000, Loss: 0.0005397242493927479\n",
      "Iteration 2500/3000, Loss: 0.0005231252289377153\n",
      "Iteration 0/3000, Loss: 4.030066967010498\n",
      "Iteration 500/3000, Loss: 0.0368545837700367\n",
      "Iteration 1000/3000, Loss: 0.010610116645693779\n",
      "Iteration 1500/3000, Loss: 0.003523604478687048\n",
      "Iteration 2000/3000, Loss: 0.0027000363916158676\n",
      "Iteration 2500/3000, Loss: 0.0021370684262365103\n",
      "Iteration 0/3000, Loss: 8.280325889587402\n",
      "Iteration 500/3000, Loss: 0.46618229150772095\n",
      "Iteration 1000/3000, Loss: 0.19832278788089752\n",
      "Iteration 1500/3000, Loss: 0.11407072842121124\n",
      "Iteration 2000/3000, Loss: 0.08800666034221649\n",
      "Iteration 2500/3000, Loss: 0.06720660626888275\n",
      "Iteration 0/3000, Loss: 0.6780884861946106\n",
      "Iteration 500/3000, Loss: 0.002157849259674549\n",
      "Iteration 1000/3000, Loss: 0.0012395386584103107\n",
      "Iteration 1500/3000, Loss: 0.0009409113554283977\n",
      "Iteration 2000/3000, Loss: 0.000848698487970978\n",
      "Iteration 2500/3000, Loss: 0.0007722377777099609\n",
      "Iteration 0/3000, Loss: 7.679708480834961\n",
      "Iteration 500/3000, Loss: 0.027866603806614876\n",
      "Iteration 1000/3000, Loss: 0.00748372683301568\n",
      "Iteration 1500/3000, Loss: 0.0037948256358504295\n",
      "Iteration 2000/3000, Loss: 0.0025160927325487137\n",
      "Iteration 2500/3000, Loss: 0.0018451721407473087\n",
      "Run 5/10\n",
      "Iteration 0/3000, Loss: 0.550766110420227\n",
      "Iteration 500/3000, Loss: 0.0060020084492862225\n",
      "Iteration 1000/3000, Loss: 0.0031863092444837093\n",
      "Iteration 1500/3000, Loss: 0.0022692596539855003\n",
      "Iteration 2000/3000, Loss: 0.001785642234608531\n",
      "Iteration 2500/3000, Loss: 0.0012519634328782558\n",
      "Iteration 0/3000, Loss: 8.96046257019043\n",
      "Iteration 500/3000, Loss: 0.04740482196211815\n",
      "Iteration 1000/3000, Loss: 0.01762581802904606\n",
      "Iteration 1500/3000, Loss: 0.01075610239058733\n",
      "Iteration 2000/3000, Loss: 0.008378462865948677\n",
      "Iteration 2500/3000, Loss: 0.005904676858335733\n",
      "Iteration 0/3000, Loss: 5.0703301429748535\n",
      "Iteration 500/3000, Loss: 0.042328301817178726\n",
      "Iteration 1000/3000, Loss: 0.009734411723911762\n",
      "Iteration 1500/3000, Loss: 0.004826303105801344\n",
      "Iteration 2000/3000, Loss: 0.003671941114589572\n",
      "Iteration 2500/3000, Loss: 0.0028266869485378265\n",
      "Iteration 0/3000, Loss: 0.6797582507133484\n",
      "Iteration 500/3000, Loss: 0.0035461680963635445\n",
      "Iteration 1000/3000, Loss: 0.0018015161622315645\n",
      "Iteration 1500/3000, Loss: 0.0012273290194571018\n",
      "Iteration 2000/3000, Loss: 0.0010632380144670606\n",
      "Iteration 2500/3000, Loss: 0.0009371376363560557\n",
      "Iteration 0/3000, Loss: 16.329130172729492\n",
      "Iteration 500/3000, Loss: 0.33741968870162964\n",
      "Iteration 1000/3000, Loss: 0.05962705239653587\n",
      "Iteration 1500/3000, Loss: 0.032275497913360596\n",
      "Iteration 2000/3000, Loss: 0.025072237476706505\n",
      "Iteration 2500/3000, Loss: 0.019439488649368286\n",
      "Iteration 0/3000, Loss: 0.09197904914617538\n",
      "Iteration 500/3000, Loss: 0.0007144273840822279\n",
      "Iteration 1000/3000, Loss: 0.0005837086937390268\n",
      "Iteration 1500/3000, Loss: 0.0005437794607132673\n",
      "Iteration 2000/3000, Loss: 0.0005314037553034723\n",
      "Iteration 2500/3000, Loss: 0.0005204580957069993\n",
      "Iteration 0/3000, Loss: 10.591522216796875\n",
      "Iteration 500/3000, Loss: 0.018071644008159637\n",
      "Iteration 1000/3000, Loss: 0.00599277438595891\n",
      "Iteration 1500/3000, Loss: 0.0033833349589258432\n",
      "Iteration 2000/3000, Loss: 0.0026601890567690134\n",
      "Iteration 2500/3000, Loss: 0.0021341063547879457\n",
      "Iteration 0/3000, Loss: 8.229429244995117\n",
      "Iteration 500/3000, Loss: 0.5177770853042603\n",
      "Iteration 1000/3000, Loss: 0.3258846402168274\n",
      "Iteration 1500/3000, Loss: 0.1476076990365982\n",
      "Iteration 2000/3000, Loss: 0.10370030999183655\n",
      "Iteration 2500/3000, Loss: 0.07429562509059906\n",
      "Iteration 0/3000, Loss: 0.10040368139743805\n",
      "Iteration 500/3000, Loss: 0.0009006077889353037\n",
      "Iteration 1000/3000, Loss: 0.0006859698914922774\n",
      "Iteration 1500/3000, Loss: 0.0006016790866851807\n",
      "Iteration 2000/3000, Loss: 0.0005753615405410528\n",
      "Iteration 2500/3000, Loss: 0.0005535133532248437\n",
      "Iteration 0/3000, Loss: 9.106505393981934\n",
      "Iteration 500/3000, Loss: 0.34875452518463135\n",
      "Iteration 1000/3000, Loss: 0.13782711327075958\n",
      "Iteration 1500/3000, Loss: 0.06058534234762192\n",
      "Iteration 2000/3000, Loss: 0.041043926030397415\n",
      "Iteration 2500/3000, Loss: 0.028897995129227638\n",
      "Run 6/10\n",
      "Iteration 0/3000, Loss: 0.25636351108551025\n",
      "Iteration 500/3000, Loss: 0.001684132614172995\n",
      "Iteration 1000/3000, Loss: 0.0010576867498457432\n",
      "Iteration 1500/3000, Loss: 0.0008377221529372036\n",
      "Iteration 2000/3000, Loss: 0.0007665606681257486\n",
      "Iteration 2500/3000, Loss: 0.0007085350807756186\n",
      "Iteration 0/3000, Loss: 11.493374824523926\n",
      "Iteration 500/3000, Loss: 0.05761478841304779\n",
      "Iteration 1000/3000, Loss: 0.014666812494397163\n",
      "Iteration 1500/3000, Loss: 0.007256014738231897\n",
      "Iteration 2000/3000, Loss: 0.005589623004198074\n",
      "Iteration 2500/3000, Loss: 0.004362664185464382\n",
      "Iteration 0/3000, Loss: 6.730632781982422\n",
      "Iteration 500/3000, Loss: 0.04739069193601608\n",
      "Iteration 1000/3000, Loss: 0.02074904926121235\n",
      "Iteration 1500/3000, Loss: 0.007323043420910835\n",
      "Iteration 2000/3000, Loss: 0.004512433893978596\n",
      "Iteration 2500/3000, Loss: 0.0030621022451668978\n",
      "Iteration 0/3000, Loss: 5.1006388664245605\n",
      "Iteration 500/3000, Loss: 0.010958384722471237\n",
      "Iteration 1000/3000, Loss: 0.0039916508831083775\n",
      "Iteration 1500/3000, Loss: 0.002358437282964587\n",
      "Iteration 2000/3000, Loss: 0.0019267174648121\n",
      "Iteration 2500/3000, Loss: 0.0016071076970547438\n",
      "Iteration 0/3000, Loss: 12.971405029296875\n",
      "Iteration 500/3000, Loss: 0.03543277829885483\n",
      "Iteration 1000/3000, Loss: 0.010449497029185295\n",
      "Iteration 1500/3000, Loss: 0.005367784295231104\n",
      "Iteration 2000/3000, Loss: 0.004140194039791822\n",
      "Iteration 2500/3000, Loss: 0.0032358230091631413\n",
      "Iteration 0/3000, Loss: 0.06007681041955948\n",
      "Iteration 500/3000, Loss: 0.0006352764321491122\n",
      "Iteration 1000/3000, Loss: 0.0005505923763848841\n",
      "Iteration 1500/3000, Loss: 0.0005181754822842777\n",
      "Iteration 2000/3000, Loss: 0.0005065312143415213\n",
      "Iteration 2500/3000, Loss: 0.0004952322342433035\n",
      "Iteration 0/3000, Loss: 5.022239685058594\n",
      "Iteration 500/3000, Loss: 0.017354965209960938\n",
      "Iteration 1000/3000, Loss: 0.005665126722306013\n",
      "Iteration 1500/3000, Loss: 0.003084592754021287\n",
      "Iteration 2000/3000, Loss: 0.0024429725017398596\n",
      "Iteration 2500/3000, Loss: 0.0019907220266759396\n",
      "Iteration 0/3000, Loss: 12.029824256896973\n",
      "Iteration 500/3000, Loss: 0.01968066580593586\n",
      "Iteration 1000/3000, Loss: 0.011891132220625877\n",
      "Iteration 1500/3000, Loss: 0.009928174316883087\n",
      "Iteration 2000/3000, Loss: 0.00910355243831873\n",
      "Iteration 2500/3000, Loss: 0.00828130729496479\n",
      "Iteration 0/3000, Loss: 0.05174161121249199\n",
      "Iteration 500/3000, Loss: 0.0007773543475195765\n",
      "Iteration 1000/3000, Loss: 0.0006067839567549527\n",
      "Iteration 1500/3000, Loss: 0.0005506800371222198\n",
      "Iteration 2000/3000, Loss: 0.0005328090628609061\n",
      "Iteration 2500/3000, Loss: 0.0005172517849132419\n",
      "Iteration 0/3000, Loss: 6.97263240814209\n",
      "Iteration 500/3000, Loss: 0.37744995951652527\n",
      "Iteration 1000/3000, Loss: 0.09462057799100876\n",
      "Iteration 1500/3000, Loss: 0.04173007979989052\n",
      "Iteration 2000/3000, Loss: 0.02979523316025734\n",
      "Iteration 2500/3000, Loss: 0.021456412971019745\n",
      "Run 7/10\n",
      "Iteration 0/3000, Loss: 0.38189461827278137\n",
      "Iteration 500/3000, Loss: 0.0015595125732943416\n",
      "Iteration 1000/3000, Loss: 0.0010524207027629018\n",
      "Iteration 1500/3000, Loss: 0.0008479146054014564\n",
      "Iteration 2000/3000, Loss: 0.0007797324797138572\n",
      "Iteration 2500/3000, Loss: 0.0007215397199615836\n",
      "Iteration 0/3000, Loss: 11.665153503417969\n",
      "Iteration 500/3000, Loss: 0.03787598758935928\n",
      "Iteration 1000/3000, Loss: 0.011109025217592716\n",
      "Iteration 1500/3000, Loss: 0.006861583795398474\n",
      "Iteration 2000/3000, Loss: 0.005706196650862694\n",
      "Iteration 2500/3000, Loss: 0.0047683995217084885\n",
      "Iteration 0/3000, Loss: 7.717784881591797\n",
      "Iteration 500/3000, Loss: 0.046315550804138184\n",
      "Iteration 1000/3000, Loss: 0.016573399305343628\n",
      "Iteration 1500/3000, Loss: 0.009498974308371544\n",
      "Iteration 2000/3000, Loss: 0.007624815683811903\n",
      "Iteration 2500/3000, Loss: 0.006165788974612951\n",
      "Iteration 0/3000, Loss: 3.979916572570801\n",
      "Iteration 500/3000, Loss: 0.01155204139649868\n",
      "Iteration 1000/3000, Loss: 0.004002217669039965\n",
      "Iteration 1500/3000, Loss: 0.002310992917045951\n",
      "Iteration 2000/3000, Loss: 0.001892636762931943\n",
      "Iteration 2500/3000, Loss: 0.0015856007812544703\n",
      "Iteration 0/3000, Loss: 10.615856170654297\n",
      "Iteration 500/3000, Loss: 0.038583993911743164\n",
      "Iteration 1000/3000, Loss: 0.011943606659770012\n",
      "Iteration 1500/3000, Loss: 0.006214079912751913\n",
      "Iteration 2000/3000, Loss: 0.004776648245751858\n",
      "Iteration 2500/3000, Loss: 0.003714792663231492\n",
      "Iteration 0/3000, Loss: 0.2453422248363495\n",
      "Iteration 500/3000, Loss: 0.0023412550799548626\n",
      "Iteration 1000/3000, Loss: 0.0009210280841216445\n",
      "Iteration 1500/3000, Loss: 0.0007190309697762132\n",
      "Iteration 2000/3000, Loss: 0.0006709366571158171\n",
      "Iteration 2500/3000, Loss: 0.0006353347562253475\n",
      "Iteration 0/3000, Loss: 4.6598687171936035\n",
      "Iteration 500/3000, Loss: 0.009427989833056927\n",
      "Iteration 1000/3000, Loss: 0.004171325825154781\n",
      "Iteration 1500/3000, Loss: 0.0025961045175790787\n",
      "Iteration 2000/3000, Loss: 0.0021392893977463245\n",
      "Iteration 2500/3000, Loss: 0.001776873366907239\n",
      "Iteration 0/3000, Loss: 12.727612495422363\n",
      "Iteration 500/3000, Loss: 0.5963142514228821\n",
      "Iteration 1000/3000, Loss: 0.3992956578731537\n",
      "Iteration 1500/3000, Loss: 0.21432949602603912\n",
      "Iteration 2000/3000, Loss: 0.15759432315826416\n",
      "Iteration 2500/3000, Loss: 0.11777544021606445\n",
      "Iteration 0/3000, Loss: 0.8231281638145447\n",
      "Iteration 500/3000, Loss: 0.002525267656892538\n",
      "Iteration 1000/3000, Loss: 0.0013137450441718102\n",
      "Iteration 1500/3000, Loss: 0.0009457655251026154\n",
      "Iteration 2000/3000, Loss: 0.0008436336647719145\n",
      "Iteration 2500/3000, Loss: 0.0007642669952474535\n",
      "Iteration 0/3000, Loss: 7.964399337768555\n",
      "Iteration 500/3000, Loss: 0.2359251230955124\n",
      "Iteration 1000/3000, Loss: 0.09397616982460022\n",
      "Iteration 1500/3000, Loss: 0.05274554342031479\n",
      "Iteration 2000/3000, Loss: 0.04089619964361191\n",
      "Iteration 2500/3000, Loss: 0.03169392794370651\n",
      "Run 8/10\n",
      "Iteration 0/3000, Loss: 0.025949547067284584\n",
      "Iteration 500/3000, Loss: 0.0006968687521293759\n",
      "Iteration 1000/3000, Loss: 0.0005804594256915152\n",
      "Iteration 1500/3000, Loss: 0.0005313914152793586\n",
      "Iteration 2000/3000, Loss: 0.000513705366756767\n",
      "Iteration 2500/3000, Loss: 0.0004972796305082738\n",
      "Iteration 0/3000, Loss: 9.468533515930176\n",
      "Iteration 500/3000, Loss: 0.04817347601056099\n",
      "Iteration 1000/3000, Loss: 0.011361408047378063\n",
      "Iteration 1500/3000, Loss: 0.005899416748434305\n",
      "Iteration 2000/3000, Loss: 0.004702300764620304\n",
      "Iteration 2500/3000, Loss: 0.003864927450194955\n",
      "Iteration 0/3000, Loss: 6.8150506019592285\n",
      "Iteration 500/3000, Loss: 0.022098828107118607\n",
      "Iteration 1000/3000, Loss: 0.0103274742141366\n",
      "Iteration 1500/3000, Loss: 0.006098894868046045\n",
      "Iteration 2000/3000, Loss: 0.004582044202834368\n",
      "Iteration 2500/3000, Loss: 0.0033025455195456743\n",
      "Iteration 0/3000, Loss: 1.7878828048706055\n",
      "Iteration 500/3000, Loss: 0.00605749012902379\n",
      "Iteration 1000/3000, Loss: 0.002602953929454088\n",
      "Iteration 1500/3000, Loss: 0.0016620841342955828\n",
      "Iteration 2000/3000, Loss: 0.0014083688147366047\n",
      "Iteration 2500/3000, Loss: 0.0012074521509930491\n",
      "Iteration 0/3000, Loss: 12.280512809753418\n",
      "Iteration 500/3000, Loss: 0.026845691725611687\n",
      "Iteration 1000/3000, Loss: 0.009553429670631886\n",
      "Iteration 1500/3000, Loss: 0.00532729085534811\n",
      "Iteration 2000/3000, Loss: 0.004206071607768536\n",
      "Iteration 2500/3000, Loss: 0.0033431495539844036\n",
      "Iteration 0/3000, Loss: 0.004651813767850399\n",
      "Iteration 500/3000, Loss: 0.0005048433085903525\n",
      "Iteration 1000/3000, Loss: 0.0004634323122445494\n",
      "Iteration 1500/3000, Loss: 0.00042345194378867745\n",
      "Iteration 2000/3000, Loss: 0.0004025344969704747\n",
      "Iteration 2500/3000, Loss: 0.00037985577364452183\n",
      "Iteration 0/3000, Loss: 2.885376453399658\n",
      "Iteration 500/3000, Loss: 0.06163845583796501\n",
      "Iteration 1000/3000, Loss: 0.025293316692113876\n",
      "Iteration 1500/3000, Loss: 0.013255697675049305\n",
      "Iteration 2000/3000, Loss: 0.01008169911801815\n",
      "Iteration 2500/3000, Loss: 0.007720372639596462\n",
      "Iteration 0/3000, Loss: 10.139385223388672\n",
      "Iteration 500/3000, Loss: 0.5358725786209106\n",
      "Iteration 1000/3000, Loss: 0.2723420262336731\n",
      "Iteration 1500/3000, Loss: 0.14615635573863983\n",
      "Iteration 2000/3000, Loss: 0.1119990348815918\n",
      "Iteration 2500/3000, Loss: 0.0872674435377121\n",
      "Iteration 0/3000, Loss: 4.4376301765441895\n",
      "Iteration 500/3000, Loss: 0.007644316181540489\n",
      "Iteration 1000/3000, Loss: 0.0030408911406993866\n",
      "Iteration 1500/3000, Loss: 0.001857269904576242\n",
      "Iteration 2000/3000, Loss: 0.0015432601794600487\n",
      "Iteration 2500/3000, Loss: 0.0013084462843835354\n",
      "Iteration 0/3000, Loss: 8.357654571533203\n",
      "Iteration 500/3000, Loss: 0.24447420239448547\n",
      "Iteration 1000/3000, Loss: 0.08215749263763428\n",
      "Iteration 1500/3000, Loss: 0.0410914421081543\n",
      "Iteration 2000/3000, Loss: 0.03034498728811741\n",
      "Iteration 2500/3000, Loss: 0.022522497922182083\n",
      "Run 9/10\n",
      "Iteration 0/3000, Loss: 3.304044723510742\n",
      "Iteration 500/3000, Loss: 0.004920945502817631\n",
      "Iteration 1000/3000, Loss: 0.002087058499455452\n",
      "Iteration 1500/3000, Loss: 0.0014219803269952536\n",
      "Iteration 2000/3000, Loss: 0.0012489603832364082\n",
      "Iteration 2500/3000, Loss: 0.0011063230922445655\n",
      "Iteration 0/3000, Loss: 15.356446266174316\n",
      "Iteration 500/3000, Loss: 0.06961701810359955\n",
      "Iteration 1000/3000, Loss: 0.021906813606619835\n",
      "Iteration 1500/3000, Loss: 0.012400072999298573\n",
      "Iteration 2000/3000, Loss: 0.009865635074675083\n",
      "Iteration 2500/3000, Loss: 0.007803189102560282\n",
      "Iteration 0/3000, Loss: 7.0876851081848145\n",
      "Iteration 500/3000, Loss: 0.032269202172756195\n",
      "Iteration 1000/3000, Loss: 0.014379972591996193\n",
      "Iteration 1500/3000, Loss: 0.00907497201114893\n",
      "Iteration 2000/3000, Loss: 0.00740564800798893\n",
      "Iteration 2500/3000, Loss: 0.006018083076924086\n",
      "Iteration 0/3000, Loss: 6.925444602966309\n",
      "Iteration 500/3000, Loss: 0.013390091247856617\n",
      "Iteration 1000/3000, Loss: 0.004648308735340834\n",
      "Iteration 1500/3000, Loss: 0.0028774936217814684\n",
      "Iteration 2000/3000, Loss: 0.0024447250179946423\n",
      "Iteration 2500/3000, Loss: 0.002078882185742259\n",
      "Iteration 0/3000, Loss: 14.123382568359375\n",
      "Iteration 500/3000, Loss: 0.06308487057685852\n",
      "Iteration 1000/3000, Loss: 0.024121107533574104\n",
      "Iteration 1500/3000, Loss: 0.013295385055243969\n",
      "Iteration 2000/3000, Loss: 0.010203721933066845\n",
      "Iteration 2500/3000, Loss: 0.007753010839223862\n",
      "Iteration 0/3000, Loss: 0.0005416274070739746\n",
      "Iteration 500/3000, Loss: 0.0003144701768178493\n",
      "Iteration 1000/3000, Loss: 0.00016269003390334547\n",
      "Iteration 1500/3000, Loss: 6.309832679107785e-05\n",
      "Iteration 2000/3000, Loss: 3.437344639678486e-05\n",
      "Iteration 2500/3000, Loss: 1.9449002138571814e-05\n",
      "Iteration 0/3000, Loss: 2.5536081790924072\n",
      "Iteration 500/3000, Loss: 0.05092649161815643\n",
      "Iteration 1000/3000, Loss: 0.020348727703094482\n",
      "Iteration 1500/3000, Loss: 0.011455118656158447\n",
      "Iteration 2000/3000, Loss: 0.007021262776106596\n",
      "Iteration 2500/3000, Loss: 0.004214604385197163\n",
      "Iteration 0/3000, Loss: 10.53099250793457\n",
      "Iteration 500/3000, Loss: 0.4951179623603821\n",
      "Iteration 1000/3000, Loss: 0.2576737105846405\n",
      "Iteration 1500/3000, Loss: 0.14803922176361084\n",
      "Iteration 2000/3000, Loss: 0.11914300918579102\n",
      "Iteration 2500/3000, Loss: 0.09436249732971191\n",
      "Iteration 0/3000, Loss: 3.6005361080169678\n",
      "Iteration 500/3000, Loss: 0.005189851857721806\n",
      "Iteration 1000/3000, Loss: 0.002621171297505498\n",
      "Iteration 1500/3000, Loss: 0.0017572303768247366\n",
      "Iteration 2000/3000, Loss: 0.00148171023465693\n",
      "Iteration 2500/3000, Loss: 0.0012624440714716911\n",
      "Iteration 0/3000, Loss: 7.436192512512207\n",
      "Iteration 500/3000, Loss: 0.2817142903804779\n",
      "Iteration 1000/3000, Loss: 0.0918024554848671\n",
      "Iteration 1500/3000, Loss: 0.04342316463589668\n",
      "Iteration 2000/3000, Loss: 0.0315629243850708\n",
      "Iteration 2500/3000, Loss: 0.02308187447488308\n",
      "Run 10/10\n",
      "Iteration 0/3000, Loss: 1.2678813934326172\n",
      "Iteration 500/3000, Loss: 0.0031219027005136013\n",
      "Iteration 1000/3000, Loss: 0.0014234730042517185\n",
      "Iteration 1500/3000, Loss: 0.0009893574751913548\n",
      "Iteration 2000/3000, Loss: 0.0008924486464820802\n",
      "Iteration 2500/3000, Loss: 0.0008245017961598933\n",
      "Iteration 0/3000, Loss: 11.885103225708008\n",
      "Iteration 500/3000, Loss: 0.05123385414481163\n",
      "Iteration 1000/3000, Loss: 0.012471110559999943\n",
      "Iteration 1500/3000, Loss: 0.0063355546444654465\n",
      "Iteration 2000/3000, Loss: 0.004952137358486652\n",
      "Iteration 2500/3000, Loss: 0.003963911905884743\n",
      "Iteration 0/3000, Loss: 6.062160015106201\n",
      "Iteration 500/3000, Loss: 0.03815745934844017\n",
      "Iteration 1000/3000, Loss: 0.018299000337719917\n",
      "Iteration 1500/3000, Loss: 0.007394027896225452\n",
      "Iteration 2000/3000, Loss: 0.0048429640009999275\n",
      "Iteration 2500/3000, Loss: 0.0034045279026031494\n",
      "Iteration 0/3000, Loss: 3.1854989528656006\n",
      "Iteration 500/3000, Loss: 0.01128621306270361\n",
      "Iteration 1000/3000, Loss: 0.005198967643082142\n",
      "Iteration 1500/3000, Loss: 0.002977942116558552\n",
      "Iteration 2000/3000, Loss: 0.002344811335206032\n",
      "Iteration 2500/3000, Loss: 0.0018616849556565285\n",
      "Iteration 0/3000, Loss: 12.133886337280273\n",
      "Iteration 500/3000, Loss: 0.021471912041306496\n",
      "Iteration 1000/3000, Loss: 0.007110574748367071\n",
      "Iteration 1500/3000, Loss: 0.003874086309224367\n",
      "Iteration 2000/3000, Loss: 0.002992721740156412\n",
      "Iteration 2500/3000, Loss: 0.0023878414649516344\n",
      "Iteration 0/3000, Loss: 0.4818509817123413\n",
      "Iteration 500/3000, Loss: 0.0327472984790802\n",
      "Iteration 1000/3000, Loss: 0.03251780569553375\n",
      "Iteration 1500/3000, Loss: 0.03245769068598747\n",
      "Iteration 2000/3000, Loss: 0.03244313225150108\n",
      "Iteration 2500/3000, Loss: 0.032430920749902725\n",
      "Iteration 0/3000, Loss: 4.404995441436768\n",
      "Iteration 500/3000, Loss: 0.010523573495447636\n",
      "Iteration 1000/3000, Loss: 0.00461700139567256\n",
      "Iteration 1500/3000, Loss: 0.0028656921349465847\n",
      "Iteration 2000/3000, Loss: 0.002336806384846568\n",
      "Iteration 2500/3000, Loss: 0.0019188704900443554\n",
      "Iteration 0/3000, Loss: 9.772958755493164\n",
      "Iteration 500/3000, Loss: 0.5715932250022888\n",
      "Iteration 1000/3000, Loss: 0.2769361436367035\n",
      "Iteration 1500/3000, Loss: 0.14941199123859406\n",
      "Iteration 2000/3000, Loss: 0.11546657234430313\n",
      "Iteration 2500/3000, Loss: 0.08889148384332657\n",
      "Iteration 0/3000, Loss: 0.022475123405456543\n",
      "Iteration 500/3000, Loss: 0.0006255499902181327\n",
      "Iteration 1000/3000, Loss: 0.0005409044097177684\n",
      "Iteration 1500/3000, Loss: 0.0005057766684331\n",
      "Iteration 2000/3000, Loss: 0.0004916790639981627\n",
      "Iteration 2500/3000, Loss: 0.00047709012869745493\n",
      "Iteration 0/3000, Loss: 7.6255269050598145\n",
      "Iteration 500/3000, Loss: 0.2165878564119339\n",
      "Iteration 1000/3000, Loss: 0.08271752297878265\n",
      "Iteration 1500/3000, Loss: 0.04233018681406975\n",
      "Iteration 2000/3000, Loss: 0.03161921724677086\n",
      "Iteration 2500/3000, Loss: 0.023634737357497215\n",
      "Results saved to ./output\\model7_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Execute the main code block for model backdoor detection\n",
    "# 'number_of_runs' is used to run the detection process multiple times to ensure consistency\n",
    "# Outputs 'anomaly_scores' and 'detection_class' for X runs, saved as CSV files for further analysis\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the available options for model_selection and model_path\n",
    "    MODEL_SELECTION_OPTIONS = {\n",
    "    1: {'model_selection': 'mnist', 'model_path': \"./model1/mnist_bd.pt\"},      # unknown backdoor model\n",
    "    2: {'model_selection': 'cifar10', 'model_path': \"./model2/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    3: {'model_selection': 'cifar10', 'model_path': \"./model3/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    4: {'model_selection': 'cifar10', 'model_path': \"./model4/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    5: {'model_selection': 'cifar10', 'model_path': \"./model5/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    6: {'model_selection': 'mnist', 'model_path': \"./model6/mnist_bd_30_c5.pt\"},      # backdoor model target class label 5 week4 exercise (30 poison test data)\n",
    "    7: {'model_selection': 'mnist', 'model_path': \"./model7/mnist_bd_50_c5.pt\"},      # backdoor model target class label 5 week4 exercise (50 poison test data)\n",
    "    8: {'model_selection': 'badnets_pytorch_mnist', 'model_path': \"./model8/badnet_MNIST.pth\"},  # badnets-pytorch backdoor model target class label 1 \n",
    "    9: {'model_selection': 'badnets_pytorch_cifar10', 'model_path': \"./model9/badnet_CIFAR10.pth\"}  # badnets-pytorch backdoor model target class label 1\n",
    "    }\n",
    "\n",
    "    selected_model_options = 7\n",
    "\n",
    "    if selected_model_options in MODEL_SELECTION_OPTIONS:\n",
    "        model_selection = MODEL_SELECTION_OPTIONS[selected_model_options]['model_selection']\n",
    "        model_path = MODEL_SELECTION_OPTIONS[selected_model_options]['model_path']\n",
    "        print(f\"Selected: Model Selection = '{model_selection}', Model Path = '{model_path}'\")\n",
    "\n",
    "    # Set the device to GPU if available, otherwise use CPU\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Active device: {device}')\n",
    "\n",
    "    # Define the number of runs for detection due to randomness\n",
    "    number_of_runs = 10  # Run detection process multiple times for consistency\n",
    "\n",
    "    # DataFrame to store results of multiple runs\n",
    "    results_list = []\n",
    "\n",
    "    # Run the backdoor detection multiple times and collect results\n",
    "    for run in range(number_of_runs):\n",
    "        print(f\"Run {run + 1}/{number_of_runs}\")\n",
    "\n",
    "        # Initialize the model and dataset\n",
    "        model, dataset = initialize_model_and_data(model_selection, model_path, device=device)\n",
    "        num_classes = 10  # Number of classes (update if different)\n",
    "\n",
    "        # Detect backdoor in the model\n",
    "        results = detect_backdoor(model, dataset, num_classes, device=device)\n",
    "        anomaly_scores = results[\"anomaly_scores\"]\n",
    "        detection_class = results.get(\"target_class\", None) if results[\"backdoor_detected\"] else None\n",
    "\n",
    "        # Save results to list\n",
    "        results_list.append({\n",
    "            'run': run + 1,\n",
    "            'class_0': anomaly_scores.get(0, None),\n",
    "            'class_1': anomaly_scores.get(1, None),\n",
    "            'class_2': anomaly_scores.get(2, None),\n",
    "            'class_3': anomaly_scores.get(3, None),\n",
    "            'class_4': anomaly_scores.get(4, None),\n",
    "            'class_5': anomaly_scores.get(5, None),\n",
    "            'class_6': anomaly_scores.get(6, None),\n",
    "            'class_7': anomaly_scores.get(7, None),\n",
    "            'class_8': anomaly_scores.get(8, None),\n",
    "            'class_9': anomaly_scores.get(9, None),\n",
    "            'target_class': detection_class\n",
    "        })\n",
    "\n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Extract filename components to construct CSV filename\n",
    "    parent_folder = os.path.basename(os.path.dirname(model_path))\n",
    "    os.makedirs('./output', exist_ok=True)\n",
    "    csv_filename = os.path.join('./output', f\"{parent_folder}_output.csv\")\n",
    "    \n",
    "    # Save DataFrame to CSV file\n",
    "    results_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Results saved to {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0s7xHqya11Fr",
    "outputId": "5ea7b3a2-6b0c-4e38-d7fa-79278b9f7719"
   },
   "outputs": [],
   "source": [
    "# # Note: This section is intended for use in Google Colab only.\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define the repository URLs and set the home directories based on Colab's file structure\n",
    "# repos = [\n",
    "#     {\"url\": \"https://github.com/noye09/cs612_SMU_g4.git\", \"target_dir\": \"/content/cs612_SMU_g4\"},\n",
    "#     {\"url\": \"https://github.com/verazuo/badnets-pytorch.git\", \"target_dir\": \"/content/cs612_SMU_g4/backdoor_cs612/badnets_pytorch\"}\n",
    "# ]\n",
    "\n",
    "# # Clone the repositories if they don't exist\n",
    "# for repo in repos:\n",
    "#     if not os.path.isdir(repo[\"target_dir\"]):\n",
    "#         print(f\"Cloning repository from {repo['url']}...\")\n",
    "#         !git clone {repo['url']}\n",
    "\n",
    "#         # Rename folder if needed (e.g., replacing hyphens with underscores)\n",
    "#         if repo[\"url\"].endswith(\"badnets-pytorch.git\"):\n",
    "#             shutil.move(\"badnets-pytorch\", repo[\"target_dir\"])\n",
    "#     else:\n",
    "#         print(f\"Repository already cloned at {repo['target_dir']}!\")\n",
    "\n",
    "# # Set the working directory to the desired location\n",
    "# HOME_DIR = \"/content/cs612_SMU_g4/backdoor_cs612/\"\n",
    "# if os.path.isdir(HOME_DIR):\n",
    "#     os.chdir(HOME_DIR)\n",
    "#     print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "# else:\n",
    "#     print(f\"Directory '{HOME_DIR}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njk_K1vw143u",
    "outputId": "27a9ac35-027d-496d-b2c1-75c1fa91cc31"
   },
   "outputs": [],
   "source": [
    "# TriggerOptimizer\n",
    "# 'initialize_model_and_data' initializes and loads the backdoor model\n",
    "# 'detect_backdoor' runs the backdoor detection process\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#import matplotlib.pyplot as plt  # For debugging and visualization\n",
    "\n",
    "from model_class.model_mnist import MNISTNet  # Import MNIST model class from model_mnist.py\n",
    "from model_class.model_cifar10 import CIFAR10Net  # Import CIFAR-10 model class from model_cifar10.py\n",
    "from model_class.badnet import BadNet\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "class TriggerOptimizer:\n",
    "    def __init__(self, model, target_class, dataset, input_shape, device='cpu', lr=0.0005):\n",
    "        # Initialize model, parameters, and optimizer settings\n",
    "        self.model = model\n",
    "        self.target_class = target_class\n",
    "        self.dataset = dataset\n",
    "        self.input_shape = input_shape\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "\n",
    "        # Initialize mask and pattern\n",
    "        self.mask = torch.rand(input_shape, requires_grad=True, device=device)\n",
    "        self.pattern = torch.rand(input_shape, requires_grad=True, device=device)\n",
    "        self.optimizer = optim.Adam([self.mask, self.pattern], lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=1500, gamma=0.5)  # Adjusted schedule\n",
    "\n",
    "    def optimize(self, max_iters=5000, patience=300, batch_size=10):\n",
    "        \"\"\"\n",
    "        Generate a minimal trigger mask and pattern for the specified target class.\n",
    "        \"\"\"\n",
    "        # Shuffle dataset to ensure diversity\n",
    "        dataset_indices = torch.randperm(len(self.dataset))\n",
    "\n",
    "        # Find multiple images belonging to the target class\n",
    "        samples = []\n",
    "        for idx in dataset_indices:\n",
    "            img, lbl = self.dataset[idx]\n",
    "            if lbl == self.target_class:\n",
    "                samples.append(img)\n",
    "            if len(samples) >= batch_size:\n",
    "                break\n",
    "\n",
    "        if len(samples) == 0:\n",
    "            raise ValueError(f\"No samples found for target class {self.target_class}\")\n",
    "\n",
    "        sample_inputs = torch.stack(samples).to(self.device)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        counter = 0\n",
    "\n",
    "        for i in range(max_iters):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Apply mask and pattern on each sample input in the batch\n",
    "            masked_inputs = self.pattern * self.mask + sample_inputs * (1 - self.mask)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(masked_inputs)  # Get predictions for all samples\n",
    "\n",
    "            # Calculate the loss (encourage model to predict the target class)\n",
    "            target_labels = torch.full((batch_size,), self.target_class, dtype=torch.long, device=self.device)\n",
    "            target_loss = nn.CrossEntropyLoss()(outputs, target_labels)\n",
    "\n",
    "            # Regularization to minimize the mask area (encourage sparse mask)\n",
    "            mask_penalty = self.mask.abs().mean()\n",
    "            total_loss = target_loss + 0.001 * mask_penalty\n",
    "\n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # Clip mask to keep values between 0 and 1\n",
    "            self.mask.data = torch.clamp(self.mask.data, 0, 1)\n",
    "\n",
    "            # Early stopping condition\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at iteration {i} due to no improvement\")\n",
    "                    break\n",
    "\n",
    "            # Print progress every 500 iterations\n",
    "            if i % 500 == 0:\n",
    "                print(f\"Iteration {i}/{max_iters}, Loss: {total_loss.item()}\")\n",
    "\n",
    "        return self.mask.detach(), self.pattern.detach()\n",
    "\n",
    "\n",
    "def initialize_model_and_data(model_selection, model_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Initializes the model and loads data based on the model selection.\n",
    "    \"\"\"\n",
    "    DATASET_OPTIONS = {\n",
    "    'mnist': {\n",
    "        'model_class': MNISTNet,\n",
    "        'dataset_class': datasets.MNIST,\n",
    "        'model_kwargs': {}\n",
    "    },\n",
    "    'cifar10': {\n",
    "        'model_class': CIFAR10Net,\n",
    "        'dataset_class': datasets.CIFAR10,\n",
    "        'model_kwargs': {}\n",
    "    },\n",
    "    'badnets_pytorch_mnist': {\n",
    "        'model_class': BadNet,\n",
    "        'dataset_class': datasets.MNIST,\n",
    "        'model_kwargs': {'input_channels': 1, 'output_num': 10}\n",
    "    },\n",
    "    'badnets_pytorch_cifar10': {\n",
    "        'model_class': BadNet,\n",
    "        'dataset_class': datasets.CIFAR10,\n",
    "        'model_kwargs': {'input_channels': 3, 'output_num': 10}\n",
    "    }\n",
    "}\n",
    "\n",
    "    # Initialize the model and load data based on the model selection\n",
    "    if model_selection not in DATASET_OPTIONS:\n",
    "        raise ValueError(f\"Invalid model selection '{model_selection}'. Available options: {list(DATASET_OPTIONS.keys())}\")\n",
    "\n",
    "    model_class = DATASET_OPTIONS[model_selection]['model_class']\n",
    "    dataset_class = DATASET_OPTIONS[model_selection]['dataset_class']\n",
    "    model_kwargs = DATASET_OPTIONS[model_selection]['model_kwargs']\n",
    "\n",
    "    # Initialize model with possible arguments\n",
    "    model = model_class(**model_kwargs)\n",
    "    test_dataset = dataset_class(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    # Load the model weights\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at path: {model_path}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)  # Send model to specified device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    return model, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "def detect_backdoor(model, dataset, num_classes, device='cpu'):\n",
    "    \"\"\"\n",
    "    Detects potential backdoors in the model using Neural Cleanse.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    input_shape = dataset[0][0].shape  # Get the shape of a single input image\n",
    "\n",
    "    results = {\n",
    "        \"backdoor_detected\": False,\n",
    "        \"anomaly_scores\": {},\n",
    "        \"triggers\": {}\n",
    "    }\n",
    "\n",
    "    mask_sizes = []\n",
    "\n",
    "    for target_class in range(num_classes):\n",
    "        try:\n",
    "            # Generate a trigger for each class\n",
    "            optimizer = TriggerOptimizer(model, target_class, dataset, input_shape, device=device)\n",
    "            mask, pattern = optimizer.optimize(max_iters=3000)\n",
    "\n",
    "            # Calculate the size of the mask (used to detect anomalies)\n",
    "            mask_size = mask.abs().mean().item()\n",
    "            mask_sizes.append(mask_size)\n",
    "\n",
    "            # Store the mask and pattern for potential backdoor classes\n",
    "            results[\"triggers\"][target_class] = {\n",
    "                \"mask\": mask.cpu().numpy(),\n",
    "                \"pattern\": pattern.cpu().numpy(),\n",
    "                \"mask_size\": mask_size\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle cases where mask generation fails\n",
    "            print(f\"Error processing target class {target_class}: {e}\")\n",
    "            mask_sizes.append(np.nan)\n",
    "            results[\"triggers\"][target_class] = {\n",
    "                \"mask\": None,\n",
    "                \"pattern\": None,\n",
    "                \"mask_size\": np.nan\n",
    "            }\n",
    "\n",
    "    # Calculate anomaly scores for each class\n",
    "    valid_mask_sizes = [x for x in mask_sizes if not np.isnan(x)]\n",
    "    mean_size = np.mean(valid_mask_sizes)\n",
    "    std_size = np.std(valid_mask_sizes)\n",
    "\n",
    "    for target_class, mask_size in enumerate(mask_sizes):\n",
    "        if not np.isnan(mask_size):\n",
    "            anomaly_score = (mask_size - mean_size) / (std_size + 1e-6)  # Avoid division by zero\n",
    "            results[\"anomaly_scores\"][target_class] = anomaly_score\n",
    "\n",
    "            # If the anomaly score is below a certain threshold, flag it as a backdoor\n",
    "            if anomaly_score < -2:  # Typically, a threshold like -2 or -3 standard deviations\n",
    "                results[\"backdoor_detected\"] = True\n",
    "                results[\"target_class\"] = target_class\n",
    "        else:\n",
    "            results[\"anomaly_scores\"][target_class] = float('inf')\n",
    "    #visualize_generated_masks(results)\n",
    "    return results\n",
    "\n",
    "# def visualize_generated_masks(results):\n",
    "#     \"\"\"\n",
    "#     Visualizes the generated masks and patterns for each target class.\n",
    "#     \"\"\"\n",
    "#     for target_class, trigger_info in results[\"triggers\"].items():\n",
    "#         if trigger_info[\"mask\"] is None:\n",
    "#             continue\n",
    "\n",
    "#         mask = trigger_info[\"mask\"]\n",
    "#         pattern = trigger_info[\"pattern\"]\n",
    "\n",
    "#         # Plot the mask and pattern\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "\n",
    "#         # Plot the mask\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         if len(mask.shape) == 3:  # If the mask has 3 dimensions\n",
    "#             mask = mask[0]  # Take the first channel (assuming it's the same across channels)\n",
    "#         plt.imshow(mask, cmap='gray')\n",
    "#         plt.title(f\"Mask for Target Class {target_class}\")\n",
    "#         plt.axis('off')\n",
    "\n",
    "#         # Plot the pattern\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         if pattern.shape[0] == 3:  # RGB Image, shape is (C, H, W), need to convert to (H, W, C)\n",
    "#             pattern = np.transpose(pattern, (1, 2, 0))  # Transpose from (C, H, W) to (H, W, C)\n",
    "#             plt.imshow(pattern)\n",
    "#         else:  # Grayscale\n",
    "#             plt.imshow(pattern.squeeze(), cmap='gray')\n",
    "#         plt.title(f\"Pattern for Target Class {target_class}\")\n",
    "#         plt.axis('off')\n",
    "\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Model Selection = 'badnets_pytorch_cifar10', Model Path = './model9/badnet_CIFAR10.pth'\n",
      "Active device: cpu\n",
      "Run 1/5\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x0a'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Initialize the model and dataset\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m model, dataset \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of classes (update if different)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Detect backdoor in the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 148\u001b[0m, in \u001b[0;36minitialize_model_and_data\u001b[1;34m(model_selection, model_path, device)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file not found at path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 148\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    149\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Send model to specified device\u001b[39;00m\n\u001b[0;32m    150\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noye\\AppData\\Local\\anaconda3\\envs\\ai_training\\Lib\\site-packages\\torch\\serialization.py:1384\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\noye\\AppData\\Local\\anaconda3\\envs\\ai_training\\Lib\\site-packages\\torch\\serialization.py:1628\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1626\u001b[0m     )\n\u001b[1;32m-> 1628\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x0a'."
     ]
    }
   ],
   "source": [
    "# Execute the main code block for model backdoor detection\n",
    "# 'number_of_runs' is used to run the detection process multiple times to ensure consistency\n",
    "# Outputs 'anomaly_scores' and 'detection_class' for X runs, saved as CSV files for further analysis\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the available options for model_selection and model_path\n",
    "    MODEL_SELECTION_OPTIONS = {\n",
    "    1: {'model_selection': 'mnist', 'model_path': \"./model1/mnist_bd.pt\"},      # unknown backdoor model\n",
    "    2: {'model_selection': 'cifar10', 'model_path': \"./model2/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    3: {'model_selection': 'cifar10', 'model_path': \"./model3/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    4: {'model_selection': 'cifar10', 'model_path': \"./model4/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    5: {'model_selection': 'cifar10', 'model_path': \"./model5/cifar10_bd.pt\"},  # unknown backdoor model\n",
    "    6: {'model_selection': 'mnist', 'model_path': \"./model6/mnist_bd_30_c5.pt\"},      # backdoor model target class label 5 week4 exercise (30 poison test data)\n",
    "    7: {'model_selection': 'mnist', 'model_path': \"./model7/mnist_bd_50_c5.pt\"},      # backdoor model target class label 5 week4 exercise (50 poison test data)\n",
    "    8: {'model_selection': 'badnets_pytorch_mnist', 'model_path': \"./model8/badnet_MNIST.pth\"},  # badnets-pytorch backdoor model target class label 0 \n",
    "    9: {'model_selection': 'badnets_pytorch_cifar10', 'model_path': \"./model9/badnet_CIFAR10.pth\"}  # badnets-pytorch backdoor model target class label 1\n",
    "    }\n",
    "\n",
    "    selected_model_options = 9\n",
    "\n",
    "    if selected_model_options in MODEL_SELECTION_OPTIONS:\n",
    "        model_selection = MODEL_SELECTION_OPTIONS[selected_model_options]['model_selection']\n",
    "        model_path = MODEL_SELECTION_OPTIONS[selected_model_options]['model_path']\n",
    "        print(f\"Selected: Model Selection = '{model_selection}', Model Path = '{model_path}'\")\n",
    "\n",
    "    # Set the device to GPU if available, otherwise use CPU\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Active device: {device}')\n",
    "\n",
    "    # Define the number of runs for detection due to randomness\n",
    "    number_of_runs = 5  # Run detection process multiple times for consistency\n",
    "\n",
    "    # DataFrame to store results of multiple runs\n",
    "    results_list = []\n",
    "\n",
    "    # Run the backdoor detection multiple times and collect results\n",
    "    for run in range(number_of_runs):\n",
    "        print(f\"Run {run + 1}/{number_of_runs}\")\n",
    "\n",
    "        # Initialize the model and dataset\n",
    "        model, dataset = initialize_model_and_data(model_selection, model_path, device=device)\n",
    "        num_classes = 10  # Number of classes (update if different)\n",
    "\n",
    "        # Detect backdoor in the model\n",
    "        results = detect_backdoor(model, dataset, num_classes, device=device)\n",
    "        anomaly_scores = results[\"anomaly_scores\"]\n",
    "        detection_class = results.get(\"target_class\", None) if results[\"backdoor_detected\"] else None\n",
    "\n",
    "        # Save results to list\n",
    "        results_list.append({\n",
    "            'run': run + 1,\n",
    "            'class_0': anomaly_scores.get(0, None),\n",
    "            'class_1': anomaly_scores.get(1, None),\n",
    "            'class_2': anomaly_scores.get(2, None),\n",
    "            'class_3': anomaly_scores.get(3, None),\n",
    "            'class_4': anomaly_scores.get(4, None),\n",
    "            'class_5': anomaly_scores.get(5, None),\n",
    "            'class_6': anomaly_scores.get(6, None),\n",
    "            'class_7': anomaly_scores.get(7, None),\n",
    "            'class_8': anomaly_scores.get(8, None),\n",
    "            'class_9': anomaly_scores.get(9, None),\n",
    "            'target_class': detection_class\n",
    "        })\n",
    "\n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Display the first 10 rows of the results\n",
    "    results_df.head(10)\n",
    "\n",
    "    # Extract filename components to construct CSV filename\n",
    "    parent_folder = os.path.basename(os.path.dirname(model_path))\n",
    "    csv_filename = os.path.join('./output', f\"{parent_folder}_output.csv\")\n",
    "    \n",
    "    # Save DataFrame to CSV file\n",
    "    results_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Results saved to {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
